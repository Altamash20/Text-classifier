{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Altamash20/Text-classifier/blob/main/TextClassification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X8WEWPT3Mq2Q"
      },
      "outputs": [],
      "source": [
        "!pip install transformers --quiet\n",
        "!pip install opendatasets --quiet\n",
        "\n",
        "import opendatasets as od\n",
        "od.download(\"https://www.kaggle.com/datasets/rmisra/news-headlines-dataset-for-sarcasm-detection\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7RsKv2wpShHF",
        "outputId": "86c2d6f5-7bcd-4c38-f259-7a9c5b4401f6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Available device:  cuda\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.optim import Adam\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "print(\"Available device: \", device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t9YYbwjzSlZo"
      },
      "outputs": [],
      "source": [
        "data_df = pd.read_json('/content/news-headlines-dataset-for-sarcasm-detection/Sarcasm_Headlines_Dataset.json', lines = True)\n",
        "data_df.dropna(inplace = True)\n",
        "data_df.drop_duplicates(inplace=True)\n",
        "data_df.drop([\"article_link\"], inplace=True, axis=1)\n",
        "print(data_df.shape)\n",
        "data_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C1aosxSkSTQu",
        "outputId": "65f4e4ab-fbe2-4d15-bfab-e165b66dc9e6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training size:  18695 rows which is: 70.0 %\n",
            "Validation size:  4006 rows which is: 15.0 %\n",
            "Testing size:  4007 rows which is: 15.0 %\n"
          ]
        }
      ],
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(np.array(data_df[\"headline\"]), np.array(data_df[\"is_sarcastic\"]), test_size=0.3)\n",
        "X_val, X_test, y_val, y_test = train_test_split(X_test, y_test, test_size = 0.5)\n",
        "\n",
        "print(\"Training size: \", X_train.shape[0], \"rows which is:\", round(X_train.shape[0]/data_df.shape[0], 4)*100, \"%\")\n",
        "print(\"Validation size: \", X_val.shape[0], \"rows which is:\", round(X_val.shape[0]/data_df.shape[0], 4)*100, \"%\")\n",
        "print(\"Testing size: \", X_test.shape[0], \"rows which is:\", round(X_test.shape[0]/data_df.shape[0], 4)*100, \"%\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ltvyjQATgqN1"
      },
      "outputs": [],
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(\"google-bert/bert-base-uncased\")\n",
        "bert_model = AutoModel.from_pretrained(\"google-bert/bert-base-uncased\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "LutwvwZIviqt"
      },
      "outputs": [],
      "source": [
        "class dataset(Dataset):\n",
        "  def __init__(self, X, Y):\n",
        "    self.X = [tokenizer(x,\n",
        "                        max_length = 100,\n",
        "                        truncation = True,\n",
        "                        padding = \"max_length\",\n",
        "                        return_tensors = 'pt').to(device)\n",
        "                        for x in X\n",
        "    ]\n",
        "    self.Y = torch.tensor(Y, dtype = torch.float32).to(device)\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.X)\n",
        "  def __getitem__(self, idx):\n",
        "    return self.X[idx], self.Y[idx]\n",
        "\n",
        "training_data = dataset(X_train, y_train)\n",
        "validation_data = dataset(X_val, y_val)\n",
        "testing_data = dataset(X_test, y_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "Zui2JPRxygxe"
      },
      "outputs": [],
      "source": [
        "from math import exp\n",
        "BATCH_SIZE = 20\n",
        "EPOCHS = 10\n",
        "LR = 1*exp(-4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "Df8AcMRd0GzG"
      },
      "outputs": [],
      "source": [
        "training_dataloader = DataLoader(training_data, batch_size=BATCH_SIZE, shuffle=True)\n",
        "validation_dataloader = DataLoader(training_data, batch_size=BATCH_SIZE, shuffle=True)\n",
        "testing_dataloader = DataLoader(training_data, batch_size=BATCH_SIZE, shuffle=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "ACVgmMxf1GlO"
      },
      "outputs": [],
      "source": [
        "class MyModel(nn.Module):\n",
        "  def __init__(self, bert):\n",
        "    super(MyModel, self).__init__()\n",
        "\n",
        "    self.bert = bert\n",
        "    self.dropout = nn.Dropout(0.25)\n",
        "    self.linear1 = nn.Linear(768, 384)\n",
        "    self.linear2 = nn.Linear(384, 1)\n",
        "    self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "  def forward(self, input_ids, attention_mask):\n",
        "    pooled_output = self.bert(input_ids, attention_mask, return_dict = False)[0][:,0]\n",
        "    output = self.linear1(pooled_output)\n",
        "    output = self.dropout(output)\n",
        "    output = self.linear2(output)\n",
        "    output = self.sigmoid(output)\n",
        "    return output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "bUMt5Fs53FKd"
      },
      "outputs": [],
      "source": [
        "for param in bert_model.parameters():\n",
        "  param.requires_grad = False\n",
        "model = MyModel(bert_model).to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G8qDoHKu4Uoh",
        "outputId": "cbc21937-b01b-496d-bfd9-1f2ddd8e5413"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "MyModel(\n",
              "  (bert): BertModel(\n",
              "    (embeddings): BertEmbeddings(\n",
              "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
              "      (position_embeddings): Embedding(512, 768)\n",
              "      (token_type_embeddings): Embedding(2, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): BertEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0-11): 12 x BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSdpaSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (pooler): BertPooler(\n",
              "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "      (activation): Tanh()\n",
              "    )\n",
              "  )\n",
              "  (dropout): Dropout(p=0.25, inplace=False)\n",
              "  (linear1): Linear(in_features=768, out_features=384, bias=True)\n",
              "  (linear2): Linear(in_features=384, out_features=1, bias=True)\n",
              "  (sigmoid): Sigmoid()\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "-StnqXJq4Yhc"
      },
      "outputs": [],
      "source": [
        "criterion = nn.BCELoss()\n",
        "optimizer = Adam(model.parameters(), lr=LR)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s8WSlf8448KK",
        "outputId": "c524d0c3-7277-4d9c-e40b-37266fb3f017"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "  Epoch No. 1 Train Loss: 13.5242 Train Accuracy: 73.993\n",
            "                      Validation Loss: 13.5242 Validation Accuracy: 373.4149\n",
            "  \n",
            "\n",
            "  Epoch No. 2 Train Loss: 24.9882 Train Accuracy: 72.827\n",
            "                      Validation Loss: 24.9882 Validation Accuracy: 249.9251\n",
            "  \n",
            "\n",
            "  Epoch No. 3 Train Loss: 51.3548 Train Accuracy: 45.0709\n",
            "                      Validation Loss: 51.3548 Validation Accuracy: 205.7414\n",
            "  \n",
            "\n",
            "  Epoch No. 4 Train Loss: 52.29 Train Accuracy: 44.076\n",
            "                      Validation Loss: 52.29 Validation Accuracy: 205.7164\n",
            "  \n",
            "\n",
            "  Epoch No. 5 Train Loss: 52.2933 Train Accuracy: 44.076\n",
            "                      Validation Loss: 52.2933 Validation Accuracy: 205.7664\n",
            "  \n",
            "\n",
            "  Epoch No. 6 Train Loss: 50.5322 Train Accuracy: 45.9267\n",
            "                      Validation Loss: 50.5322 Validation Accuracy: 206.3155\n",
            "  \n",
            "\n",
            "  Epoch No. 7 Train Loss: 52.17 Train Accuracy: 44.2043\n",
            "                      Validation Loss: 52.17 Validation Accuracy: 206.2406\n",
            "  \n",
            "\n",
            "  Epoch No. 8 Train Loss: 51.6023 Train Accuracy: 44.7927\n",
            "                      Validation Loss: 51.6023 Validation Accuracy: 363.9041\n",
            "  \n",
            "\n",
            "  Epoch No. 9 Train Loss: 40.3454 Train Accuracy: 56.8013\n",
            "                      Validation Loss: 40.3454 Validation Accuracy: 344.6081\n",
            "  \n",
            "\n",
            "  Epoch No. 10 Train Loss: 23.05 Train Accuracy: 75.3036\n",
            "                      Validation Loss: 23.05 Validation Accuracy: 313.8542\n",
            "  \n"
          ]
        }
      ],
      "source": [
        "total_loss_train_plot = []\n",
        "total_loss_validation_plot = []\n",
        "total_acc_train_plot = []\n",
        "total_acc_validation_plot = []\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "  total_acc_train = 0\n",
        "  total_loss_train = 0\n",
        "  total_acc_val = 0\n",
        "  total_loss_val = 0\n",
        "  for idx, data in enumerate(training_dataloader):\n",
        "    inputs, labels = data\n",
        "    inputs.to(device)\n",
        "    labels.to(device)\n",
        "\n",
        "    prediction = model(inputs[\"input_ids\"].squeeze(1), inputs[\"attention_mask\"].squeeze(1)).squeeze(1)\n",
        "    batch_loss = criterion(prediction, labels)\n",
        "    total_loss_train += batch_loss.item()\n",
        "\n",
        "    acc = (prediction.round() == labels).sum().item()\n",
        "\n",
        "    total_acc_train += acc\n",
        "\n",
        "    batch_loss.backward()\n",
        "    optimizer.step()\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "  with torch.no_grad():\n",
        "    for idx, data in enumerate(validation_dataloader):\n",
        "      inputs, labels = data\n",
        "      inputs.to(device)\n",
        "      labels.to(device)\n",
        "\n",
        "      prediction = model(inputs[\"input_ids\"].squeeze(1), inputs[\"attention_mask\"].squeeze(1)).squeeze(1)\n",
        "      batch_loss = criterion(prediction, labels)\n",
        "      total_loss_val += batch_loss.item()\n",
        "\n",
        "      acc = (prediction.round()==labels).sum().item()\n",
        "\n",
        "      total_acc_val += acc\n",
        "\n",
        "  total_loss_train_plot.append(round(total_loss_train/1000, 4))\n",
        "  total_loss_validation_plot.append(round(total_loss_train/1000, 4))\n",
        "  total_acc_train_plot.append(round((total_acc_train/training_data.__len__())*100, 4))\n",
        "  total_acc_validation_plot.append(round((total_acc_val/validation_data.__len__())*100, 4))\n",
        "\n",
        "  print(f\"\"\"\n",
        "  Epoch No. {epoch+1} Train Loss: {round(total_loss_train/1000, 4)} Train Accuracy: {round((total_acc_train/training_data.__len__())*100, 4)}\n",
        "                      Validation Loss: {round(total_loss_train/1000, 4)} Validation Accuracy: {round((total_acc_val/validation_data.__len__())*100, 4)}\n",
        "  \"\"\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mpfygyamQ75L"
      },
      "outputs": [],
      "source": [
        "with torch.no_grad():\n",
        "  total_loss_test = 0\n",
        "  total_acc_test = 0\n",
        "\n",
        "  for idx, data in enumerate(testing_dataloader):\n",
        "    inputs, labels = data\n",
        "    inputs.to(device)\n",
        "    labels.to(device)\n",
        "\n",
        "    prediction = model(inputs[\"input_ids\"].squeeze(1), inputs[\"attention_mask\"].squeeze(1)).squeeze(1)\n",
        "    batch_loss = criterion(prediction, labels)\n",
        "    total_loss_val += batch_loss.item()\n",
        "\n",
        "    acc = (prediction.round()==labels).sum().item()\n",
        "\n",
        "    total_acc_val += acc\n",
        "print(f\"Accuracy Score on testing Data is: {round((total_acc_test/testing_data.__len__())*100, 4)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ay6v4aFnVLf-"
      },
      "outputs": [],
      "source": [
        "fig, axs = plt.subplots(nrows = 1, ncols = 2, figsize = (15, 5))\n",
        "\n",
        "axs[0].plot(total_loss_train_plot, label = 'Training Loss')\n",
        "axs[0].plot(total_loss_validation_plot, label = 'Validation Loss')\n",
        "axs[0].set_title('Training and Validation Loss over Epochs')\n",
        "axs[0].set_xlabel('Epochs')\n",
        "axs[0].set_ylabel('Loss')\n",
        "axs[0].set_ylim([0, 1])\n",
        "axs[0].legend()\n",
        "\n",
        "\n",
        "axs[1].plot(total_acc_train_plot, label = 'Training Accuracy')\n",
        "axs[1].plot(total_acc_validation_plot, label = 'Validation Accuracy')\n",
        "axs[1].set_title('Training and Validation Accuracy over Epochs')\n",
        "axs[1].set_xlabel('Epochs')\n",
        "axs[1].set_ylabel('Accuracy')\n",
        "axs[1].set_ylim([0, 100])\n",
        "axs[1].legend()\n",
        "\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNbHkVExPhHKizkf84Zy8Mn",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}